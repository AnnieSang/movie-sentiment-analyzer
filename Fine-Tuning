Let‚Äôs make a super simple Python example for fine-tuning a model using external data, suitable for an old computer, so it won‚Äôt require a GPU.

We‚Äôll use Hugging Face + a small text classification task. You‚Äôll see how to:

Load an external dataset
Fine-tune a small pre-trained model (DistilBERT) (what DistilBERT for? -- Annie)
Test it
This uses only a small subset of data so it runs on CPU.

Python Example: Fine-Tuning DistilBERT on IMDB Reviews (Fine-Tuning DistilBERT on IMDB (Internet Movie Database))

# Install required packages if not installed

# pip install torch transformers datasets sklearn

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# ------------------- 1Ô∏è‚É£ Load External Dataset -------------------
dataset = load_dataset("imdb")

# For a small demo, we only use 500 training and 100 test samples
train_dataset = dataset['train'].shuffle(seed=42).select(range(500))
test_dataset = dataset['test'].shuffle(seed=42).select(range(100))

# ------------------- 2Ô∏è‚É£ Load Pretrained Tokenizer + Model -------------------
model_name = "distilbert-base-uncased"  # small, CPU-friendly model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# ------------------- 3Ô∏è‚É£ Tokenize Dataset -------------------
def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# Set format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# ------------------- 4Ô∏è‚É£ Define Metrics -------------------
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# ------------------- 5Ô∏è‚É£ Training Arguments -------------------
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,          # just 1 epoch for demo
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy="epoch",
    logging_steps=10,
    save_strategy="no",          # don't save checkpoints for small demo
    learning_rate=2e-5,
    report_to="none"             # disable wandb logging
)

# ------------------- 6Ô∏è‚É£ Trainer -------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# ------------------- 7Ô∏è‚É£ Train -------------------
trainer.train()

# ------------------- 8Ô∏è‚É£ Evaluate -------------------
results = trainer.evaluate()
print("Evaluation results:", results)

What this example does:

Loads external data (IMDB movie reviews) from Hugging Face datasets.
Loads a pre-trained small model (distilbert-base-uncased).
Fine-tunes the model on your custom subset (500 reviews).
Evaluates accuracy, F1 score, precision, recall on a test subset.
CPU-friendly ‚Äî you can run this on a low-power computer.
üí° Notes:

You don‚Äôt need a GPU, but training is faster with one.
You can replace IMDB with any small CSV file:
from datasets import load_dataset
dataset = load_dataset('csv', data_files={'train':'train.csv','test':'test.csv'})

This teaches you how models learn from external data and how to fine-tune pre-trained models.
